# -*- coding: utf-8 -*-
"""TEXT CLASSIFICATION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wKwW_gNb_0Ywe9KbczV0x7U046xJaCKh

# ***IMPORTING OF THE NECESSARY LIBRARIES***
"""

import nltk
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.decomposition import LatentDirichletAllocation as LDA
from neupy import algorithms, utils

"""***DOWNLOADING RELEVANT RESOURCES FROM NLTK***"""

nltk.download('gutenberg')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

"""***CHECKING THE CONTENT OF GUTENBERG***"""

nltk.corpus.gutenberg.fileids()

"""***ASSIGNING THE SELECTED TEXTS TO VARIABLES***"""

text1 =  nltk.corpus.gutenberg.raw('austen-emma.txt')
text2 = nltk.corpus.gutenberg.raw('austen-sense.txt')
text3 = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')
text4 = nltk.corpus.gutenberg.raw('edgeworth-parents.txt')
text5 = nltk.corpus.gutenberg.raw('bible-kjv.txt')
text6 = nltk.corpus.gutenberg.raw('chesterton-ball.txt')
text7 = nltk.corpus.gutenberg.raw('chesterton-brown.txt')

print(text1)

"""***VIEWING THE NUMBER OF WORDS IN EACH TEXT***"""

number_words_texts=[(len(nltk.word_tokenize(text1)),'austen-emma'),(len(nltk.word_tokenize(text2)),'bible-kjv'), (len(nltk.word_tokenize(text3)),'whitman-leaves'), 
                    (len(nltk.word_tokenize(text4)),'milton-paradise'), (len(nltk.word_tokenize(text5)),'melville-moby_dick'), 
                    (len(nltk.word_tokenize(text6)),'edgeworth-parents'), (len(nltk.word_tokenize(text7)),'chesterton-thursday'), 
                    (len(nltk.word_tokenize(text8)),'shakespeare-hamlet'), (len(nltk.word_tokenize(text9)),'bryant-stories')]
print(number_words_texts)

n_o_w = pd.DataFrame(number_words_texts)
n_o_w = n_o_w.rename(columns={0: "Number of words", 1: "Text"})
print(n_o_w)

sns.barplot(n_o_w['Text'],n_o_w['Number of words'],label="Number of Words")
plt.xticks(rotation=90)
plt.show()

"""***TOKENIZING THE SELECTED TEXTS INTO SENTENCES***"""

sent1 = nltk.sent_tokenize(text1)
sent2 = nltk.sent_tokenize(text2)
sent3 = nltk.sent_tokenize(text3)
sent4 = nltk.sent_tokenize(text4)
sent5 = nltk.sent_tokenize(text5)
sent6 = nltk.sent_tokenize(text6)
sent7 = nltk.sent_tokenize(text7)

print(sent3)

"""## ***PREPROCESSING OF THE DATA***

***DEFINING A FUNCTION (word_list) TO REMOVE STOPWORDS, NUMBERS, PUNCTUATIONS AS WELL AS LEMMATIZE***
"""

wordlemmatize = WordNetLemmatizer()

def word_list(sent):
    sentn = ''
    sentn = sentn.join(sent)
    sentn=sentn.replace('.',' ').replace(',',' ').replace('!',' ').replace('?',' ').replace('--',' ').replace('-',' ').replace(';',' ').replace("'",' ').replace('"',' ').replace("_",' ').replace(':',' ').replace('(',' ').replace(')',' ').replace('0',' ').replace('1',' ').replace('2',' ').replace('3',' ').replace('4',' ').replace('5',' ').replace('6',' ').replace('7',' ').replace('8',' ').replace('9',' ')#REPLACING OF UNWANTED CHARACTERS
    words = nltk.word_tokenize(sentn) #TOKENIZING THE SENTENCES INTO WORDS
    stop_words = set(stopwords.words('english'))

    wordlen = []
    words = [w.lower() for w in words if not w in stop_words] #REMOVAL OF STOPWORDS FROM THE TEXT 
    for word in words:
      wordsv = wordlemmatize.lemmatize(word, pos='v') #LEMMATIZING OF THE TEXT BASED ON VERBS
      wordsa = wordlemmatize.lemmatize(wordsv, pos='a') #LEMMATIZING OF THE TEXT BASED ON ADJECTIVES
      words = wordlemmatize.lemmatize(wordsa, pos='n') #LEMMATIZING OF THE TEXT BASED ON NOUNS
      wordlen.append(words) #APPENDING THE WORDS AFTER THE REMOVAL OF STOPWORDS, UNWANTED PUNTUATIONS, SYMBOLS, NUMBERS AND LEMMANTIZING 
    return wordlen

"""***APPLYING THE FUNCTION (word_list) TO THE TOKENIZED SENTENCES***"""

words1 = word_list(sent1)
words2 = word_list(sent2)
words3 = word_list(sent3)
words4 = word_list(sent4)
words5 = word_list(sent5)
words6 = word_list(sent6)
words7 = word_list(sent7)

"""***CHECKING FOR THE LENGHT OF THE TEXTS AFTER THE FIRST FUNTION WAS APPLIED***"""

number_words_texts=[(len(words1),'austen-emma'),(len(words2),'bible-kjv'),(len(words3),'whitman-leaves'), (len(words4), 'milton-paradise'),
                    (len(words5),'melville-moby_dick'), (len(words6), 'edgeworth-parents'), (len(words7),'chesterton-thursday'),
                    (len(words8),'shakespeare-hamlet'),(len(words9),'bryant-stories')]
n_o_w = pd.DataFrame(number_words_texts)
n_o_w = n_o_w.rename(columns={0: "Number of words", 1: "Text"})
print(n_o_w)

"""***VISUALIZATION OF THE LENGHT OF THE TEXTS***"""

sns.barplot(n_o_w['Text'],n_o_w['Number of words'],label="Number of Words")
plt.xticks(rotation=90)
plt.show()

"""***DEFINING ANOTHER FUNTION (random_sample) TO ASSIGN 100 WORDS TO A DOCUMENT AND PICK 150 RANDOM DOCUMENTS FROM EACH TEXT***"""

def random_sample(list_words):
  count = 0
  str = ''
  for word in list_words:
    if count<150:
      str = str + word
      str = str+' '
      count = count + 1
    else:
      str = str+'###'
      count = 0
  strr = str.split('###')
  return random.sample(strr,200)

"""***APPLYING OF THE SECOND FUNCTION (random_sample) TO THE PROCESSED TEXTS***"""

r_str1 = random_sample(words1)
r_str2 = random_sample(words2)
r_str3 = random_sample(words3)
r_str4 = random_sample(words4)
r_str5 = random_sample(words5)
r_str6 = random_sample(words6)
r_str7 = random_sample(words7)

print(r_str1[4])

"""***ASSIGNING THE NAME OF THE AUTHOR OF EACH DOCUMENT TO THE RESPECTIVE DOCUMENT***"""

labeled_names = ([(sent, 'austen') for sent in r_str1] + [(sent, 'austen') for sent in r_str2]+[(sent, 'mellville') for sent in r_str3]+
                 [(sent, 'edgeworth') for sent in r_str4]+[(sent, 'Bible') for sent in r_str5]+[(sent, 'chesterton') for sent in r_str6]+
								         [(sent, 'chesterton') for sent in r_str7])
pd.DataFrame(labeled_names)

"""***SHUFFLING THE LABELED NAMES AND VIEWING THEM IN A DATAFRAME USING PANDAS***"""

random.shuffle(labeled_names)
labeled = pd.DataFrame(labeled_names)

"""***LABELING OF THE COLUMNS OF THE DATAFRAME***"""

labeled = labeled.rename(columns={0: "Text", 1: "Author"})
labeled.head()

"""***SPLITTING OF THE DATASET***"""

X = labeled['Text'].values
y = labeled['Author'].values

pd.DataFrame(X).head()

"""## **FEATURE ENGINEERING**
***TRANSFORMATION OF THE TEXT USING BAG OF WORDS***
"""

count = CountVectorizer(min_df=3, analyzer='word', ngram_range=(1,2))#, max_features=5000) #CONSIDERING BOTH BIGRAMS AND UNIGRAMS, IGNORING WORDS THAT HAVE A DOCUMENT FREQUENCY OF LESS THAN 3 AND CONSIDERING THE TOP 5000 FEATURES BASED ON FREQUENCY ACROSS THE CORPUS.
bow = count.fit_transform(X)
bow.toarray()

Bow_feature_names = count.get_feature_names()

X_Bow = pd.DataFrame(bow.toarray(), columns=Bow_feature_names) #VIEWING IN FORM OF A DATAFRAME

X_Bow

"""***TRANSFORMATION OF THE TEXT USING TF-IDF***"""

tf = TfidfVectorizer(analyzer='word',min_df= 3, ngram_range=(1, 2))#, max_features=5000)
Tfid = tf.fit_transform(X)
Tfid.toarray()

tfid_feature_names = tf.get_feature_names()
X_Tfid = pd.DataFrame(Tfid.toarray(), columns=tfid_feature_names)
X_Tfid

lda = LDA(n_components=5, n_jobs=-1)
X_lda = lda.fit_transform(X_Bow)

X_Tlda = pd.DataFrame(X_lda)
X_Tlda

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_lda,y, test_size=0.20, random_state=5)

kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(X_train)

from sklearn.metrics import homogeneity_score
homogeneity_score(y_test, kmeans.predict(X_test))

from sklearn.metrics import silhouette_score
silhouette_score(X_test, labels=kmeans.predict(X_test))

plt.scatter(X_train[pred_y == 0, 0], X_train[pred_y == 0, 1], s = 20, c = 'red')
plt.scatter(X_train[pred_y == 1, 0], X_train[pred_y == 1, 1], s = 20, c = 'blue')
plt.scatter(X_train[pred_y == 2, 0], X_train[pred_y == 2, 1], s = 20, c = 'green')
plt.scatter(X_train[pred_y == 3, 0], X_train[pred_y == 3, 1], s = 20, c = 'cyan')
plt.scatter(X_train[pred_y == 4, 0], X_train[pred_y == 4, 1], s = 20, c = 'magenta')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 50, c = 'black', label = 'Centroids')

# import hierarchical clustering libraries
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(X_train, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
y_hc = hc.fit_predict(X_train)

"""## ***MODELLING***"""

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
rf_reg = RandomForestClassifier()

scores = []
for i in range(10):
    result = next(kf.split(X_Bow), None)
    X_train = X_Bow.iloc[result[0]]
    X_test = X_Bow.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = rf_reg.fit(X_train,y_train)
    predictions = rf_reg.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
RandomForest_Bow = np.mean(scores)
print('Average K-Fold(Random Forest- BOW) Score :' , RandomForest_Bow)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
rf_reg = RandomForestClassifier()

scores = []
for i in range(10):
    result = next(kf.split(X_Tfid), None)
    X_train = X_Tfid.iloc[result[0]]
    X_test = X_Tfid.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = rf_reg.fit(X_train,y_train)
    predictions = rf_reg.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
RandomForest_Tfid = np.mean(scores)
print('Average K-Fold Score(Random Forest- TFid) :' , RandomForest_Tfid)

f = KFold(n_splits = 10, shuffle = False, random_state=None)
clf = SVC()

scores = []
for i in range(10):
    result = next(kf.split(X_Bow), None)
    X_train = X_Bow.iloc[result[0]]
    X_test = X_Bow.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = clf.fit(X_train,y_train)
    predictions = clf.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
SVM_Bow = np.mean(scores)
print('Average K-Fold Score(SVM- BOW) :' , SVM_Bow)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
clf = SVC()

scores = []
for i in range(10):
    result = next(kf.split(X_Tfid), None)
    X_train = X_Tfid.iloc[result[0]]
    X_test = X_Tfid.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = clf.fit(X_train,y_train)
    predictions = clf.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
SVM_Tfid = np.mean(scores)
print('Average K-Fold Score(SVM- TFid) :' , SVM_Tfid)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
dtc = DecisionTreeClassifier()

scores = []
for i in range(10):
    result = next(kf.split(X_Bow), None)
    X_train = X_Bow.iloc[result[0]]
    X_test = X_Bow.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = dtc.fit(X_train,y_train)
    predictions = dtc.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
DecisionTree_Bow = np.mean(scores)
print('Average K-Fold Score(Decision Tree- BOW) :' , DecisionTree_Bow)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
dtc = DecisionTreeClassifier()

scores = []
for i in range(10):
    result = next(kf.split(X_Tfid), None)
    X_train = X_Tfid.iloc[result[0]]
    X_test = X_Tfid.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = dtc.fit(X_train,y_train)
    predictions = dtc.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
DecisionTree_Tfid = np.mean(scores)
print('Average K-Fold Score(Decision Tree- TFid) :' , DecisionTree_Tfid)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
neigh = KNeighborsClassifier(n_neighbors=3)

scores = []
for i in range(10):
    result = next(kf.split(X_Bow), None)
    X_train = X_Bow.iloc[result[0]]
    X_test = X_Bow.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = neigh.fit(X_train,y_train)
    predictions = neigh.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
KNN_Bow = np.mean(scores)
print('Average K-Fold Score(KNN- BOW) :' , KNN_Bow)

kf = KFold(n_splits = 10, shuffle = False, random_state=None)
neigh = KNeighborsClassifier(n_neighbors=3)

scores = []
for i in range(10):
    result = next(kf.split(X_Tfid), None)
    X_train = X_Tfid.iloc[result[0]]
    X_test = X_Tfid.iloc[result[1]]
    y_train = y[result[0]]
    y_test = y[result[1]]
    model = neigh.fit(X_train,y_train)
    predictions = neigh.predict(X_test)
    scores.append(model.score(X_test,y_test))
print('Scores from each Iteration: ', scores)
KNN_Tfid = np.mean(scores)
print('Average K-Fold Score(KNN- TFid) :' , KNN_Tfid)

entries = [('RandomForest', RandomForest_Tfid),('SVM', SVM_Tfid), ('Decision Tree', DecisionTree_Tfid), ('KNN', KNN_Tfid)]
cv_df = pd.DataFrame(entries, columns=['model_name', 'accuracy'])
import seaborn as sns
sns.boxplot(x='model_name', y='accuracy', data=cv_df)
sns.stripplot(x='model_name', y='accuracy', data=cv_df, 
              size=8, jitter=True, edgecolor="gray", linewidth=2)
plt.show()

entries = [('RandomForest', RandomForest_Bow),('SVM', SVM_Bow), ('Decision Tree', DecisionTree_Bow), ('KNN', KNN_Bow)]
cv_df = pd.DataFrame(entries, columns=['model_name', 'accuracy'])
import seaborn as sns
sns.boxplot(x='model_name', y='accuracy', data=cv_df)
sns.stripplot(x='model_name', y='accuracy', data=cv_df, 
              size=8, jitter=True, edgecolor="gray", linewidth=2)
plt.show()

"""### ***MODEL EVALUATION***
***SVM - TF-IDF***
"""

model = SVC()

X_train,X_test,y_train,y_test,indices_train, indices_test = train_test_split(X_Tfid,y,labeled.index, test_size=0.30, random_state=5)

model.fit(X_train,y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

conf_mat = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=auth.Author.values, yticklabels=auth.Author.values)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.yticks(rotation=90)
plt.xticks(rotation=90)
plt.show()

"""***ERROR ANALYSIS***"""

encoding = [(0,'austen'),(1,'bible'),(2,'whitman'),(3,'milton'),(4,'melville'),(5, 'edgeworth'),(6,'chesterton'),(7,'shakespear'),(8,'bryant')]
auth = pd.DataFrame(encoding)
auth = auth.rename(columns={0: "Label", 1: "Author"})
print(auth)

for predicted in auth.Label:
  for actual in auth.Label:
    if predicted != actual and conf_mat[actual, predicted] >= 1:
      print("'{}' predicted as '{}' : {} examples.".format(y_test[actual],y_pred[predicted], conf_mat[actual, predicted]))
      display(labeled.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Author', 'Text']])
      #print(y_test.tolist().index(actual))
      print('')

import joblib
filename = 'finalized_model.sav'
joblib.dump(model, filename)

